{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "453565e6",
   "metadata": {},
   "source": [
    "# Implementing UMAP from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c31a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from scipy.optimize import curve_fit\n",
    "import numba\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "sns.set_theme()\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc269e",
   "metadata": {},
   "source": [
    "## Load the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits data\n",
    "mnist = fetch_openml('mnist_784', parser='auto')\n",
    "\n",
    "# Normalize digits data\n",
    "mnist_data = mnist.data/255\n",
    "mnist_label = mnist.target.astype(object)\n",
    "\n",
    "# Downsample\n",
    "mnist_label = mnist_label[(mnist_label.isin(['0', '1', '2', '3', '4']))]\n",
    "\n",
    "mnist_data = mnist_data.loc[mnist_label.index, :].reset_index(drop=True)\n",
    "\n",
    "mnist_label = mnist_label.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ea338",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65206fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the first instance of each number 0-4\n",
    "first_instance = {}\n",
    "\n",
    "# Loop through the dataset to find the first instance for numbers 0-4\n",
    "for i in range(len(mnist_label)):\n",
    "    \n",
    "    label = mnist_label[i]\n",
    "    \n",
    "    if label not in first_instance:\n",
    "        first_instance[label] = i\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "\n",
    "for label, index in first_instance.items():\n",
    "    ax = axes[int(label)]\n",
    "    ax.imshow(mnist_data.loc[index].values.reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dadccd",
   "metadata": {},
   "source": [
    "## Building a graph\n",
    "\n",
    "\n",
    "### Find nearest neighbours and distances for each point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28146c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = 20\n",
    "index = NNDescent(mnist_data, n_neighbors=N_NEIGHBORS)\n",
    "neighbors, distances = index.query(mnist_data, k=N_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neighbors[:20,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distances[:20,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2acb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = neighbors[:, 1:]\n",
    "distances = distances[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(distances.shape[1], dtype=np.int32)+1, distances[0,:])\n",
    "plt.xlabel('Nth nearest neighbor')\n",
    "plt.xticks(np.arange(distances.shape[1], dtype=np.int32)+1)\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Neighbor distance from \"data point 0\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a9b97",
   "metadata": {},
   "source": [
    "### Calculate rho for each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660e2f9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho_i = \\min_{j \\in S} d(i, j)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhos = distances[:, 0]\n",
    "print(rhos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44821ccb",
   "metadata": {},
   "source": [
    "### Calculate sigmas\n",
    "\n",
    "\n",
    "$$\n",
    "sim_{ij} = \\exp \\left(- \\frac{\\max(0, d(x_i, x_{ij}) - \\rho_i)}{\\sigma_i} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma_i = \\text{the value such that } \\sum_{j=1}^{k} \\exp \\left(- \\frac{\\max(0, d(x_i, x_{ij}) - \\rho_i)}{\\sigma_i} \\right) = \\log_2(k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter values for binary search\n",
    "BINARY_ITER = 64\n",
    "GAMMA_TOLERANCE = 1e-5\n",
    "\n",
    "# Log target is same everywhere\n",
    "log_target = np.log2(N_NEIGHBORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc929fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - binary search for a single row\n",
    "curr_dist = distances[0,:]\n",
    "\n",
    "# Initialize array of zeros\n",
    "zero_array = np.zeros(curr_dist.shape[0])\n",
    "\n",
    "# Set up binary search for sigma\n",
    "sigma_high = np.inf\n",
    "sigma_low = 0\n",
    "sigma_mid = 1\n",
    "\n",
    "# Perform N itterations\n",
    "for iter_i in range(BINARY_ITER):\n",
    "    \n",
    "    # Substract distances and rho \n",
    "    dist_diff = curr_dist - rhos[0]\n",
    "\n",
    "    # Divide by sigma, negate, perform exponent, sum\n",
    "    sigma_sum = np.sum(np.exp(-dist_diff/sigma_mid))\n",
    "    \n",
    "    # Break the loop if sum is equal to the target\n",
    "    if np.abs(sigma_sum-log_target) < GAMMA_TOLERANCE:\n",
    "        break\n",
    "    \n",
    "    # If obtained sum is greater than the target then\n",
    "    # make sigma smaller\n",
    "    if sigma_sum > log_target:\n",
    "        \n",
    "        sigma_high = sigma_mid\n",
    "        sigma_mid = (sigma_high + sigma_low)/2\n",
    "    \n",
    "    # If obtained sum is lower than the target then\n",
    "    # make sigma higher\n",
    "    else:\n",
    "        sigma_low = sigma_mid\n",
    "        if sigma_high == np.inf:\n",
    "            sigma_mid = sigma_mid*2\n",
    "        else:\n",
    "            sigma_mid = (sigma_high + sigma_low)/2\n",
    "            \n",
    "print('Obtained sigma sum is : {}'.format(sigma_sum))\n",
    "print('Target sigma sum is : {}'.format(log_target))\n",
    "print('Value of sigma is : {}'.format(sigma_mid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8510695",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_1 = np.exp(-((curr_dist - rhos[0]) / (sigma_mid)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(distances.shape[1], dtype=np.int32)+1, sims_1)\n",
    "plt.xlabel('Nth nearest neighbor')\n",
    "plt.xticks(np.arange(distances.shape[1], dtype=np.int32)+1)\n",
    "plt.ylabel('Similarity')\n",
    "plt.title('Neighbor similarity to \"data point 0\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap binary search into function\n",
    "def binary_search_for_sigma(curr_dist, curr_rho, log_target, binary_iter, smooth_k_tolerance):\n",
    "    \n",
    "    # Set up binary search for sigma\n",
    "    sigma_high = np.inf\n",
    "    sigma_low = 0\n",
    "    sigma_mid = 1\n",
    "\n",
    "    # Perform N itterations\n",
    "    for _ in range(binary_iter):\n",
    "\n",
    "        # Substract distances and rho \n",
    "        dist_diff = curr_dist - curr_rho\n",
    "\n",
    "        # Divide by sigma, negate, perform exponent, sum\n",
    "        sigma_sum = np.sum(np.exp(-dist_diff/sigma_mid))\n",
    "\n",
    "        # Break the loop if sum is equal to the target\n",
    "        if np.abs(sigma_sum-log_target) < smooth_k_tolerance:\n",
    "            return sigma_mid\n",
    "\n",
    "        # If obtained sum is greater than the target then\n",
    "        # make sigma smaller\n",
    "        if sigma_sum > log_target:\n",
    "\n",
    "            sigma_high = sigma_mid\n",
    "            sigma_mid = (sigma_high + sigma_low)/2\n",
    "\n",
    "        # If obtained sum is lower than the target then\n",
    "        # make sigma higher\n",
    "        else:\n",
    "            sigma_low = sigma_mid\n",
    "            if sigma_high == np.inf:\n",
    "                sigma_mid = sigma_mid*2\n",
    "            else:\n",
    "                sigma_mid = (sigma_high + sigma_low)/2\n",
    "\n",
    "    return sigma_mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22113aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sigma value for each sample\n",
    "sigma_values = np.zeros(distances.shape[0])\n",
    "\n",
    "for sample_i in range(distances.shape[0]):\n",
    "    \n",
    "    sigma_values[sample_i] = binary_search_for_sigma(\n",
    "        curr_dist=distances[sample_i,:], \n",
    "        curr_rho=rhos[sample_i], \n",
    "        log_target=log_target,\n",
    "        binary_iter=BINARY_ITER,\n",
    "        smooth_k_tolerance=GAMMA_TOLERANCE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a93d534",
   "metadata": {},
   "source": [
    "### Calculate similarities between points\n",
    "\n",
    "Similarities to neighbours are calculated as:\n",
    "\n",
    "$$\n",
    "sim_{ij} = \\exp \\left(- \\frac{\\max(0, d(x_i, x_{ij}) - \\rho_i)}{\\sigma_i} \\right)\n",
    "$$\n",
    "\n",
    "This similarities are \"simetrized\" after calculation.\n",
    "\n",
    "Formula for all the samples:\n",
    "$$\n",
    "S = P + P^T - P * P^T\n",
    "$$\n",
    "\n",
    "Example formula for two samples:\n",
    "$$\n",
    "S = P_(ij) + P_(ji) - P_(ij) * P_(ji)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of samples and choosen number of neighbours\n",
    "n_samples = neighbors.shape[0]\n",
    "n_neighbors = neighbors.shape[1]\n",
    "\n",
    "# Initialize row, col and val arrays, this will later\n",
    "# be used to construct sparse matrix in order to symetrize\n",
    "# smoothed distance matrix.\n",
    "row_idx = np.zeros(neighbors.size, dtype=np.int32) # will be used to store index of the \n",
    "col_idx = np.zeros(neighbors.size, dtype=np.int32) # will be used to store index of the neighbours\n",
    "mtx_vals = np.zeros(neighbors.size, dtype=np.float32) # will be used to store similarity between sample and neighboor\n",
    "\n",
    "# Loop goint through samples\n",
    "for i in range(n_samples):\n",
    "    # Inner loop going through the neighbours\n",
    "    for j in range(n_neighbors):\n",
    "        \n",
    "        # Calculate similarities in high dimensional space\n",
    "        val = np.exp(-((distances[i, j] - rhos[i]) / (sigma_values[i])))\n",
    "        \n",
    "        # store sample indexes e.g. [1,1,1,1,2,2,2,2]\n",
    "        row_idx[i * n_neighbors + j] = i\n",
    "        # store neighbour indexes e.g. [2,3,4,5,2,3,4,5]\n",
    "        col_idx[i * n_neighbors + j] = neighbors[i, j]\n",
    "        # store similarities e.g [0.1, 0.3, 0.2, 0.5, 0.8,0.12, 0.19, 0.97]\n",
    "        mtx_vals[i * n_neighbors + j] = val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b35489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse matrix of smoothed distances\n",
    "# Distances in this matrix are not yet symetric\n",
    "non_sym_sim_mtx = scipy.sparse.coo_matrix(\n",
    "    (mtx_vals, (row_idx, col_idx)), shape=(neighbors.shape[0], neighbors.shape[0])\n",
    ")\n",
    "non_sym_sim_mtx.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063de39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sym_sim_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose smooth distance matrix\n",
    "non_sym_sim_mtx_t = non_sym_sim_mtx.transpose()\n",
    "\n",
    "# Multiply smooth distance matrix with its transponse element-wise\n",
    "prod_matrix = non_sym_sim_mtx.multiply(non_sym_sim_mtx_t)\n",
    "\n",
    "# Create symetrical smooth distance matrix by using fuzzy set union operation\n",
    "sim_mtx = non_sym_sim_mtx + non_sym_sim_mtx_t - prod_matrix\n",
    "\n",
    "# Eliminate all zeroes from the result\n",
    "sim_mtx.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a114a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform sparse matrix to coordinate format\n",
    "sim_mtx = sim_mtx.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return to the old format\n",
    "sim_mtx_row_ids = sim_mtx.row\n",
    "sim_mtx_col_ids = sim_mtx.col\n",
    "sim_mtx_vals = sim_mtx.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how indexes and coordinates look like\n",
    "# Also show identity of each index (for both samples and neighbours)\n",
    "test = pd.DataFrame(\n",
    "    np.column_stack([sim_mtx_row_ids, sim_mtx_col_ids, sim_mtx_vals]),\n",
    "    columns = ['row', 'col', 'similarities']\n",
    ")\n",
    "\n",
    "ri_labels = mnist_label.reset_index(drop=True)\n",
    "row_labels = ri_labels[sim_mtx_row_ids]\n",
    "col_labels = ri_labels[sim_mtx_col_ids]\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97407989",
   "metadata": {},
   "source": [
    "## Embedding the graph into 2D space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a0033",
   "metadata": {},
   "source": [
    "### Calculate parameters a and b\n",
    "\n",
    "In low dimensional space, similarity scores are calculated based on following formula:\n",
    "\n",
    "$$\n",
    "Sim(x, y) = \\frac{1}{1 + a \\cdot d^{(2 \\cdot b)}}\n",
    "$$\n",
    "\n",
    "\n",
    "Parameters a and b are optimized so Sim(x, y) approximates following values:\n",
    "\n",
    "$$\n",
    "Target vals(x, y) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } d \\le \\text{min-dist} \\\\\n",
    "\\exp\\left(-\\left(d - \\text{min-dist}\\right)\\right), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18145d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyper parameters\n",
    "MIN_DIST = 0.1\n",
    "N_COMPONENTS = 2\n",
    "N_EPOCHS = 2000\n",
    "N_NEGATIVE = 8\n",
    "LEARNING_RATE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_curve(x, a, b):\n",
    "    return 1.0 / (1.0 + a * x ** (2 * b))\n",
    "    \n",
    "# 300 simulated points between 0 and 3\n",
    "# this axis simulates distances between\n",
    "# points in low dimensional space\n",
    "sim_dist = np.linspace(0, 3, 300)\n",
    "# placehodler for target valyes \n",
    "target_vals = np.zeros(sim_dist.shape)\n",
    "# set distances smaller than min_dist to 1\n",
    "target_vals[sim_dist < MIN_DIST] = 1.0\n",
    "# set distances greater than min_dist\n",
    "target_vals[sim_dist >= MIN_DIST] = np.exp(-(sim_dist[sim_dist >= MIN_DIST] - MIN_DIST)\n",
    ")\n",
    "# optimize a and b parameters of function defined by \"optim_curve\"\n",
    "# in order to produce target_vals when sim_dist is inputed\n",
    "params, _ = curve_fit(optim_curve, sim_dist, target_vals)\n",
    "\n",
    "a,b = params[0], params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Value of a is {}'.format(a))\n",
    "print('Value of b is {}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7aec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sim_dist, target_vals)\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Similarity')\n",
    "plt.title('Target similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc937d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dist = np.linspace(0, 3, 300)\n",
    "similarities = optim_curve(sim_dist, a, b)\n",
    "plt.plot(sim_dist, similarities)\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Similarity')\n",
    "plt.title('Fitted curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4c0fe",
   "metadata": {},
   "source": [
    "### Low dimensional embedding optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding with random values\n",
    "embedding = np.random.uniform(\n",
    "    low=-3.0, high=3.0, size=(sim_mtx.shape[0], N_COMPONENTS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_optim_one_epoch(sim_mtx_row_ids, sim_mtx_col_ids, a, b, embedding, weights, learning_rate, n_negative):\n",
    "    \n",
    "    # Iterate through pairs of samples and neighboirs \n",
    "    # i represents sample index\n",
    "    # j represents neighbour index\n",
    "    for i,j in zip(sim_mtx_row_ids, sim_mtx_col_ids):\n",
    "        \n",
    "        # Get embeddings for sample and index\n",
    "        p1 = embedding[i, :]\n",
    "        p2 = embedding[j, :]\n",
    "        \n",
    "        # Find euclidean distance in low dimensional space\n",
    "        dist_squared = np.sum(np.square(p1-p2))\n",
    "        \n",
    "        # Do full gradient calculation for neighbours if distance is greater than zero\n",
    "        if dist_squared > 0:\n",
    "            curr_grad = (\n",
    "                weights[i]*2*a*b*dist_squared**(b-1) / (a*dist_squared**b + 1.) \n",
    "                - (1-weights[i])*(2*b / ((0.001 + dist_squared)*(1+a*dist_squared**b)))\n",
    "            )\n",
    "        else:\n",
    "            curr_grad = 0.\n",
    "        \n",
    "        curr_grad = curr_grad*(p1-p2)\n",
    "        \n",
    "        # Clip the gradients\n",
    "        curr_grad = np.clip(curr_grad, -4., 4.)\n",
    "        \n",
    "        # Apply gradient to both, sample an neighbour\n",
    "        embedding[i, :] -= curr_grad*learning_rate\n",
    "        embedding[j, :] += curr_grad*learning_rate\n",
    "        \n",
    "        # Sample N random points and use them as \"negative sample\"\n",
    "        # Negative sample is the sample that is not connected to the current sample\n",
    "        for _ in range(n_negative):\n",
    "            \n",
    "            # Get randomly sample index\n",
    "            neg_j = np.random.randint(embedding.shape[0])\n",
    "            \n",
    "            # Get embedding of negative sample\n",
    "            p_neg = embedding[neg_j, : ]\n",
    "            \n",
    "            # Find euclidean distance between current sample and \n",
    "            # negative sample\n",
    "            dist_squared = np.sum(np.square(p1-p_neg))\n",
    "            \n",
    "            # Calculate gradient (formula is shorter because we assume weights[i]==0)\n",
    "            # for thus case\n",
    "            if dist_squared > 0.:\n",
    "                curr_grad = -2.*b / ((0.001 + dist_squared)*(1+a*dist_squared**b))\n",
    "            else:\n",
    "                curr_grad = 0.\n",
    "\n",
    "            curr_grad = curr_grad*(p1-p_neg)\n",
    "            \n",
    "            # Clip the gradient\n",
    "            curr_grad = np.clip(curr_grad, -4., 4.)\n",
    "            \n",
    "            # Apply gradient to the both, sample and the neighbour\n",
    "            embedding[i, :] -=  curr_grad*learning_rate\n",
    "            embedding[neg_j, :] +=  curr_grad*learning_rate\n",
    "\n",
    "\n",
    "# Optimize the function by using numba\n",
    "optimize_fn = numba.njit(\n",
    "    euclidean_optim_one_epoch, fastmath=True, parallel=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189db8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_optim_one_epoch(\n",
    "    sim_mtx_row_ids=sim_mtx_row_ids, \n",
    "    sim_mtx_col_ids=sim_mtx_col_ids, \n",
    "    a=a, \n",
    "    b=b, \n",
    "    embedding=embedding, \n",
    "    weights=sim_mtx_vals,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_negative=N_NEGATIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ab708",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_fn(\n",
    "    sim_mtx_row_ids=sim_mtx_row_ids, \n",
    "    sim_mtx_col_ids=sim_mtx_col_ids, \n",
    "    a=a, \n",
    "    b=b, \n",
    "    embedding=embedding, \n",
    "    weights=sim_mtx_vals,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_negative=N_NEGATIVE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9221f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize in a loop\n",
    "for epoch_i in range(200):\n",
    "    optimize_fn(\n",
    "        sim_mtx_row_ids=sim_mtx_row_ids, \n",
    "        sim_mtx_col_ids=sim_mtx_col_ids, \n",
    "        a=a, \n",
    "        b=b, \n",
    "        embedding=embedding, \n",
    "        weights=sim_mtx_vals,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        n_negative=N_NEGATIVE\n",
    "    )\n",
    "    \n",
    "    if epoch_i%20 == 0:\n",
    "        print(epoch_i)\n",
    "        test_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2'])\n",
    "        test_df['label'] = mnist_label.values\n",
    "        sns.scatterplot(test_df,x='UMAP1', y='UMAP2', hue='label')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694edcc",
   "metadata": {},
   "source": [
    "### Alternative function implementation with numba parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_optim_one_epoch(sim_mtx_row_ids, sim_mtx_col_ids, a, b, embedding, weights, learning_rate, n_negative):\n",
    "\n",
    "    # In order to uze numpa parallel, loop must be performed with numba.prange\n",
    "    for iter_i in numba.prange(sim_mtx_row_ids.shape[0]):\n",
    "        \n",
    "        # Extract index for sample and neighbour\n",
    "        i = sim_mtx_row_ids[iter_i]\n",
    "        j = sim_mtx_col_ids[iter_i]\n",
    "\n",
    "        # Get embeddings for sample and neighbour\n",
    "        p1 = embedding[i, :]\n",
    "        p2 = embedding[j, :]\n",
    "        \n",
    "        # Find euclidean distance in low dimensional space\n",
    "        dist_squared = np.sum(np.square(p1-p2))\n",
    "        \n",
    "        # Do full gradient calculation for neighbours if distance is greater than zero\n",
    "        if dist_squared > 0:\n",
    "            curr_grad = (\n",
    "                weights[i]*2*a*b*dist_squared**(b-1) / (a*dist_squared**b + 1.) \n",
    "                - (1-weights[i])*(2*b / ((0.001 + dist_squared)*(1+a*dist_squared**b)))\n",
    "            )\n",
    "        else:\n",
    "            curr_grad = 0.\n",
    "        \n",
    "        curr_grad = curr_grad*(p1-p2)\n",
    "        \n",
    "        # Clip the gradients\n",
    "        curr_grad = np.clip(curr_grad, -4., 4.)\n",
    "        \n",
    "        # Apply gradient to both, sample an neighbour\n",
    "        embedding[i, :] -= curr_grad*learning_rate\n",
    "        embedding[j, :] += curr_grad*learning_rate\n",
    "        \n",
    "        # Sample N random points and use them as \"negative sample\"\n",
    "        # Negative sample is the sample that is not connected to the current sample\n",
    "        for _ in range(n_negative):\n",
    "            \n",
    "            # Get randomly sample index\n",
    "            neg_j = np.random.randint(embedding.shape[0])\n",
    "            \n",
    "            # Get embedding of negative sample\n",
    "            p_neg = embedding[neg_j, : ]\n",
    "            \n",
    "            # Find euclidean distance between current sample and \n",
    "            # negative sample\n",
    "            dist_squared = np.sum(np.square(p1-p_neg))\n",
    "            \n",
    "            # Calculate gradient (formula is shorter because we assume weights[i]==0)\n",
    "            # for thus case\n",
    "            if dist_squared > 0.:\n",
    "                curr_grad = -2.*b / ((0.001 + dist_squared)*(1+a*dist_squared**b))\n",
    "            else:\n",
    "                curr_grad = 0.\n",
    "\n",
    "            curr_grad = curr_grad*(p1-p_neg)\n",
    "            \n",
    "            # Clip the gradient\n",
    "            curr_grad = np.clip(curr_grad, -4., 4.)\n",
    "            \n",
    "            # Apply gradient to the both, sample and the neighbour\n",
    "            embedding[i, :] -=  curr_grad*learning_rate\n",
    "            embedding[neg_j, :] +=  curr_grad*learning_rate\n",
    "\n",
    "# Optimize the function            \n",
    "optimize_fn = numba.njit(\n",
    "    euclidean_optim_one_epoch, fastmath=True, parallel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd302a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding with random values\n",
    "embedding = np.random.uniform(\n",
    "    low=-3.0, high=3.0, size=(sim_mtx.shape[0], N_COMPONENTS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b37ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize in a loop\n",
    "for epoch_i in range(N_EPOCHS):\n",
    "    optimize_fn(\n",
    "        sim_mtx_row_ids=sim_mtx_row_ids, \n",
    "        sim_mtx_col_ids=sim_mtx_col_ids, \n",
    "        a=a, \n",
    "        b=b, \n",
    "        embedding=embedding, \n",
    "        weights=sim_mtx_vals,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        n_negative=N_NEGATIVE\n",
    "    )\n",
    "    \n",
    "    if epoch_i%100 == 0:\n",
    "        print(epoch_i)\n",
    "        test_df = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2'])\n",
    "        test_df['label'] = mnist_label.values\n",
    "        sns.scatterplot(test_df,x='UMAP1', y='UMAP2', hue='label', alpha=0.8, s=8)\n",
    "        plt.title('Epoch ' + str(epoch_i))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0bf36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
