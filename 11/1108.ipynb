{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louvain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "ig.config['plotting.backend']='matplotlib'\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "random.seed(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load karate club dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the karate club dataset\n",
    "g = ig.Graph.Famous('Zachary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.is_weighted())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of nodes\n",
    "num_nodes = g.vcount()\n",
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of edges\n",
    "num_edges= g.ecount()\n",
    "num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weights to the edges\n",
    "g.es['weight'] = [1]*num_edges\n",
    "g.vs['member_id'] = range(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list = [(edge.source, edge.target) for edge in g.es]\n",
    "edges_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g.layout(\"auto\")\n",
    "ig.plot(\n",
    "    g, \n",
    "    layout=layout, \n",
    "    vertex_label=g.vs[\"member_id\"],\n",
    "    vertex_size=0.3, vertex_label_size=15\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Louvain implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjacency matrix\n",
    "adj_matrix = np.array(g.get_adjacency(attribute=\"weight\").data)\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get node degrees\n",
    "node_degrees = np.sum(adj_matrix, axis=1)\n",
    "node_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degrees = np.expand_dims(node_degrees, axis=-1)\n",
    "node_degrees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_sum = np.sum(node_degrees)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_modularity(adj_matrix, node_degrees, cluster_ids, edge_sum, resolution=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates resolution modified modularity.\n",
    "\n",
    "    :param adj_matrix: Adjacency matrix,\n",
    "    :param node_degrees: Array containing node degress.\n",
    "    :param cluster_ids: Array containing cluster ids.\n",
    "    :param edge_sum: Total edge sum.\n",
    "    :param resolution: Resolution parameter, defaults to 1\n",
    "    :return: Resolution modified modularity.\n",
    "    \"\"\"\n",
    "\n",
    "    mod = 0\n",
    "    \n",
    "    # Find modularity for each cluster\n",
    "    for cluster in np.unique(cluster_ids):\n",
    "        \n",
    "        # Select all nodes belonging to current cluster\n",
    "        curr_clust_ids = np.where(cluster_ids==cluster)[0]\n",
    "        curr_adj = adj_matrix[np.ix_(curr_clust_ids, curr_clust_ids)]\n",
    "        curr_deg = node_degrees[curr_clust_ids, :]\n",
    "        \n",
    "        mod+= (np.sum(curr_adj) - resolution*np.sum(curr_deg@curr_deg.T)/(edge_sum*2))/(edge_sum*2)\n",
    "        \n",
    "    return mod\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_change(curr_node, target_cluster, adj_matrix, cluster_ids, edge_sum, resolution=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Change in modularity when we assign standalone curr_node to the target_cluster.\n",
    "\n",
    "    :param curr_node: Id of the node we consider moving to neighboring communities\n",
    "    :param target_cluster: Cluster we are considering moving the node into\n",
    "    :param adj_matrix: Adjacency matrix.\n",
    "    :param cluster_ids: Array containing cluster if for each node.\n",
    "    :param edge_sum: Total edge sum.\n",
    "    :param resolution: Resolution, defaults to 1.\n",
    "    :return: Change in modularity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get target community nodes\n",
    "    target_cluster_nodes = np.argwhere(\n",
    "        cluster_ids==target_cluster\n",
    "    ).flatten()\n",
    "\n",
    "    # Weights of all edges connected to curr_node\n",
    "    k_adj = adj_matrix[curr_node, : ]\n",
    "    \n",
    "    # Degree of curr node\n",
    "    k_i = k_adj.sum()\n",
    "    \n",
    "    # Sigma_tot - weight of all edges that are connected \n",
    "    # to the target cluster nodes\n",
    "    k_i_in = k_adj[target_cluster_nodes].sum()*2\n",
    "    sigma_tot = adj_matrix[target_cluster_nodes, :].sum()\n",
    "\n",
    "    # change in modularity\n",
    "    mod_change = k_i_in/(2*edge_sum) - resolution*sigma_tot*k_i/(2*edge_sum**2)\n",
    "    \n",
    "    return mod_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "RESOLUTION = 0.4\n",
    "THRESHOLD = 1e-7\n",
    "\n",
    "# Set initial cluster ids and adj matrix\n",
    "cluster_ids = np.array(range(num_nodes))\n",
    "adj_matrix = np.array(g.get_adjacency(attribute=\"weight\").data)\n",
    "\n",
    "# Prepare dataframe to store cluster ids\n",
    "cluster_df = pd.DataFrame(index=range(adj_matrix.shape[0]))\n",
    "cluster_df[0] = range(adj_matrix.shape[0])\n",
    "\n",
    "col_counter = 1\n",
    "\n",
    "# Louvain clustering iterations\n",
    "while True:\n",
    "    \n",
    "    print('\\n')\n",
    "    print('New epoch starting')\n",
    "    \n",
    "    node_degrees = np.sum(adj_matrix, axis=1)\n",
    "    node_degrees = np.expand_dims(node_degrees, axis=-1)\n",
    "    edge_sum = np.sum(node_degrees)/2\n",
    "    \n",
    "    modularity = calc_modularity(\n",
    "        adj_matrix=adj_matrix,\n",
    "        node_degrees=node_degrees,\n",
    "        cluster_ids=cluster_ids,\n",
    "        edge_sum=edge_sum,\n",
    "        resolution=RESOLUTION\n",
    "    )\n",
    "    \n",
    "    start_cluster_ids = cluster_ids.copy()\n",
    "    start_modularity = modularity.copy()\n",
    "    \n",
    "    # Modularity optimization\n",
    "    while True:\n",
    "                \n",
    "        modularity_improved = False\n",
    "        \n",
    "        # Each iteration requires shuffled node list\n",
    "        node_list = list(range(num_nodes))\n",
    "        random.shuffle(node_list)\n",
    "        \n",
    "        print('Startin pass through the nodes.')\n",
    "        # Iterate through all the nodes\n",
    "        for node_i in node_list:\n",
    "            \n",
    "            # Get node neighbors, exclude the node itself\n",
    "            node_neighbors = np.where(adj_matrix[node_i]>0)[0]\n",
    "            node_neighbors = [neigh for neigh in node_neighbors if neigh != node_i]\n",
    "            \n",
    "            # Get neighboring communities\n",
    "            neighbor_clusters = cluster_ids[node_neighbors]\n",
    "            neighbor_clusters_unique = np.unique(neighbor_clusters)\n",
    "            \n",
    "            # Save node cluster identity\n",
    "            node_cluster = cluster_ids[node_i]\n",
    "            \n",
    "            # Determine if node is a singleton node\n",
    "            singleton_node = node_cluster not in neighbor_clusters \n",
    "            \n",
    "            # Make node identity \"-1\" - make it standalone node\n",
    "            cluster_ids[node_i] = -1\n",
    "            \n",
    "            # Dict that stores modularity improvement when this \n",
    "            # standalone node is assigned to neighboring communities\n",
    "            modularity_improvements = {}\n",
    "            \n",
    "            # If node is already singleton, adding the node to its \n",
    "            # own community brings no change in modularity\n",
    "            if singleton_node:\n",
    "                modularity_improvements[node_cluster] = 0\n",
    "            \n",
    "            # Iterate through neighboring communities\n",
    "            for neighbor_cluster in neighbor_clusters_unique:\n",
    "                \n",
    "                mod_change = modularity_change(\n",
    "                    curr_node=node_i,\n",
    "                    target_cluster=neighbor_cluster,\n",
    "                    adj_matrix=adj_matrix,\n",
    "                    cluster_ids=cluster_ids,\n",
    "                    edge_sum=edge_sum,\n",
    "                    resolution=RESOLUTION\n",
    "                )\n",
    "                \n",
    "                modularity_improvements[neighbor_cluster] = mod_change\n",
    "                                    \n",
    "            base_change = modularity_improvements.pop(node_cluster)\n",
    "            \n",
    "            # Detect community yielding highest modularity increase\n",
    "            # Handle case when node is not connected to other communities\n",
    "            if len(modularity_improvements) > 0:\n",
    "                max_neigh, max_mod_chg = max(modularity_improvements.items(), key=lambda x: x[1])\n",
    "            else:\n",
    "                max_neigh, max_mod_chg = -1, -np.inf\n",
    "            \n",
    "            # Update node community, update modularity\n",
    "            if max_mod_chg > base_change:\n",
    "                \n",
    "                cluster_ids[node_i] = max_neigh\n",
    "                modularity += max_mod_chg - base_change\n",
    "                modularity_improved = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                cluster_ids[node_i] = node_cluster\n",
    "\n",
    "        # If modularity was not improved during current pass,\n",
    "        # end the modularity optimization step              \n",
    "        if not modularity_improved:\n",
    "            break\n",
    "        \n",
    "    print('Starting resolution modified modularity is {}'.format(start_modularity))\n",
    "    print('End resolution modified modularity is {}'.format(modularity))\n",
    "    print('Num clusters is {}'.format(np.unique(cluster_ids).shape[0]))\n",
    "\n",
    "    # If modularity was not improved enough during modularity\n",
    "    # improvement step, stop the algorithm\n",
    "    if start_modularity + THRESHOLD >= modularity:\n",
    "        print('DONE !')\n",
    "        break\n",
    "        \n",
    "    print('Merging communities')\n",
    "    \n",
    "    # Remap cluster ids to original node ids & perform aggregation\n",
    "    \n",
    "    # Get unique cluster ids, and give them new IDs 0:num_clust\n",
    "    unique_clusters = np.unique(cluster_ids)\n",
    "    cluster_map = {unique_clusters[i]: i for i in range(unique_clusters.shape[0])}\n",
    "    cluster_ids = [cluster_map[clust_id] for clust_id in cluster_ids]\n",
    "    \n",
    "    # Assign new cluster id to each point\n",
    "    multi_map_dict = {start_cluster_id: cluster_id for start_cluster_id, cluster_id in zip(start_cluster_ids, cluster_ids)}\n",
    "    cluster_df[col_counter] = cluster_df[col_counter-1].map(multi_map_dict)\n",
    "    col_counter += 1\n",
    "    \n",
    "    # Get new unique clusters in aggregated points\n",
    "    unique_clusters = np.unique(cluster_ids)\n",
    "                \n",
    "    # Merge nodes\n",
    "    new_adj_matrix = np.zeros([unique_clusters.shape[0], unique_clusters.shape[0]])\n",
    "    \n",
    "    # Populate the adjacency matrix\n",
    "    for clust_i in range(unique_clusters.shape[0]):\n",
    "        \n",
    "        for clust_j in range(clust_i, unique_clusters.shape[0]):\n",
    "            \n",
    "            clust_i_ids = np.where(cluster_ids==unique_clusters[clust_i])[0]\n",
    "            clust_j_ids = np.where(cluster_ids==unique_clusters[clust_j])[0]\n",
    "            \n",
    "            if clust_i == clust_j:\n",
    "                new_adj_matrix[clust_i, clust_j] = np.sum(adj_matrix[np.ix_(clust_i_ids, clust_j_ids)])\n",
    "            else:\n",
    "                new_adj_matrix[clust_i, clust_j] = np.sum(adj_matrix[np.ix_(clust_i_ids, clust_j_ids)])\n",
    "                new_adj_matrix[clust_j, clust_i] = new_adj_matrix[clust_i, clust_j]\n",
    "                \n",
    "    # Initialize new cluster ids\n",
    "    adj_matrix = new_adj_matrix.copy()\n",
    "    \n",
    "    cluster_ids = np.arange(adj_matrix.shape[0])\n",
    "    num_nodes = adj_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g.layout(\"auto\")\n",
    "ig.plot(\n",
    "    g, \n",
    "    layout=layout, \n",
    "    vertex_label=cluster_df[2].values, \n",
    "    vertex_size=0.3, \n",
    "    vertex_label_size=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_clusters = g.community_multilevel(resolution=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ig_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_clusters.modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_clusters.graph == g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.plot(ig_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_cluster_ids = ig_clusters.membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = g.layout(\"auto\")\n",
    "ig.plot(g, layout=layout, vertex_label=ig_cluster_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
